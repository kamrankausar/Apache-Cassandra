{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parts\n",
    "# 1. Installation\n",
    "# 2. Checking\n",
    "# 3. Then at last cassandar spark connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 2 Checking\n",
    "# I am using the spark default shell\n",
    "# Got to the bin dir of spark and run ./spark-shell\n",
    "# run this command \"sc.parallelize(1 to 50).sum()\"\n",
    "# If the output come correct then all things is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part Spark Cassandra connector\n",
    "# You have to download the cassandar spark connector from the below ling\n",
    "# https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector\n",
    "# After download place this jar as usuall in the spark jar folder i.e spark/jar/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Left part of connection i.e How to use\n",
    "# start the shell with this command:  ../bin/spark-shell –jars ~loaction_of_Spark/jars/spark-cassandra-connector-assembly\n",
    "# Then \n",
    "# Connect the Spark Context to the Cassandra cluster: \n",
    "# \n",
    "#    1. Stop the default context:\n",
    "#    sc.stop\n",
    "#    2. Import the necessary jar files:\n",
    "#    import com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConf\n",
    "#    3. Make a new SparkConf with the Cassandra connection details:\n",
    "#   \n",
    "#    val conf = new SparkConf(true).set(“spark.cassandra.connection.host”, “localhost”)\n",
    "#    4. Create a new Spark Context:\n",
    "#    val sc = new SparkContext(conf)\n",
    "#    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last part i.e checking\n",
    "#Test that the Spark Connector is working from the Shell\n",
    "\n",
    "#    1. Create a keyspace called “test_spark” in Cassandra\n",
    "#    create the table test_spark.test (value int PRIMARY KEY); in the test_spark keyspace\n",
    "#    Insert some data (INSERT INTO test_spark (value) VALUES (1);)\n",
    "#    2. From the Spark Shell run the following commands:\n",
    "#        val test_spark_rdd = sc.cassandraTable(“test_spark”, “test”)\n",
    "#        test_spark_rdd.first\n",
    "#   3. The output should be:\n",
    "#        res1: com.datastax.spark.connector.CassandraRow = CassandraRow{value: 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More is on \n",
    "# https://www.datastax.com/dev/blog/kindling-an-introduction-to-spark-with-cassandra-part-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
